##################### Training Control #####################
deterministic: False
use_compile: True
inference_mode: False # If this one is True, use inference
seed: 1234
max_epochs: 40

##################### Distributed Training Control #####################
devices: 1
num_nodes: 1
strategy: auto

##################### Dataset Setting #####################
dataset_class_name: representation
dataset_dir: dataset/N-ImageNet
representation_names: ['time_surface']
split_ratios: [0.8, 0.1, 0.1]
batch_size: 2
test_batch_size: 8
height: 640
width: 480
num_workers: 8
persistent_workers: True
use_cache: True
cache_root: dataset/N-ImageNetCache
purpose: train
time_surface_use_polarity: True
time_surface_tau: 0.5
time_surface_normalize: 'standardization'
time_surface_spatial_downsample_ratio: 1.0

##################### Model Architecture #####################
model_class_name: unimodal
in_channels: 2
backbone_arch: convnext_base
encoder_only: False
drop_path_rate: 0.1
patch_size: None
proj_dim: 128
decoder_out_channels: 2

##################### Optimizers and Loss Functions #####################
weight_decay: 1e-6 # Default L2 Regularization
lr: 1e-3
lr_scheduler: step
lr_decay_epochs: 8
lr_decay_rate: 0.5
lr_decay_min_lr: 1e-6

###################### Tensorboard Logger Setting #####################
log_dir_root: 'lightning_logs'
experiment_name: 'time_surface_unimodal'

##################### Checkpoint & Restart Control #####################
enable_checkpointing: True